{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1687787597342
        }
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import requests, uuid, json\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1687787599042
        }
      },
      "outputs": [],
      "source": [
        "#Openai key here\n",
        "#Azure openai have api call like this\n",
        "openai.api_type = \"###\"\n",
        "openai.api_base = \"###\"\n",
        "openai.api_version = \"###\"\n",
        "openai.api_key = \"###\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1687787600702
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Construct connection string\n",
        "host = \"HOST_NAME\"\n",
        "dbname = \"DATABASE_TYPE\"\n",
        "user = \"USER\"\n",
        "password = \"PASSWORD\"\n",
        "sslmode = \"###\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1687787604150
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection established\n"
          ]
        }
      ],
      "source": [
        "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
        "conn = psycopg2.connect(conn_string) \n",
        "print(\"Connection established\")\n",
        "cursor = conn.cursor()\n",
        "# Fetch all rows from table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Chatbot():\n",
        "    def __init__(self):\n",
        "        self.memory = [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"\"}]\n",
        "        self.Lawyer_prompt = \"LawBot, a GPT-based AI assistant designed specifically for answering legal questions and explaining complex legal terms.\\\n",
        "        Analysis the Question sector and provide a comprehensive and briefly answer in Answer section.The answer must address all aspects\\\n",
        "        of the Question and support your argument with appropriate legal references in Context section. Only answered by thai language only \"\n",
        "    \n",
        "    def thai2eng(self ,input):### I use azure ai translated\n",
        "        key = \"\"\n",
        "        endpoint = \"\"\n",
        "        location = \"\"\n",
        "        path = \"\"\n",
        "        constructed_url = endpoint + path\n",
        "        params = {\n",
        "            'api-version': '3.0',\n",
        "            'from': 'th',\n",
        "            'to': ['en']\n",
        "        }\n",
        "        headers = {\n",
        "            'Ocp-Apim-Subscription-Key': key,\n",
        "            'Ocp-Apim-Subscription-Region': location,\n",
        "            'Content-type': 'application/json',\n",
        "            'X-ClientTraceId': str(uuid.uuid4())\n",
        "        }\n",
        "        body = [{\n",
        "            'text': input\n",
        "        }]\n",
        "        request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "        response = request.json()\n",
        "        return response[0][\"translations\"][0]['text']\n",
        "\n",
        "    def text2vec(self, input):\n",
        "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')#Your prefer LM \n",
        "        embed_list = []\n",
        "        if torch.cuda.is_available():  \n",
        "            model = model.to('cuda')  \n",
        "            embeddings = model.encode(input, device='cuda')\n",
        "            embed_list.append(embeddings.tolist())\n",
        "        return embed_list[0]\n",
        "\n",
        "    def search_vectordb(self, question):\n",
        "        search_text = []\n",
        "        try:\n",
        "            cursor.execute(f\"\"\"SELECT text_thai, cosine_distance('{self.text2vec(question)}', embedding) as cosign\n",
        "            FROM Criminal_Law\n",
        "            ORDER BY cosine_distance('{self.text2vec(question)}', embedding) ASC LIMIT 7\n",
        "            \"\"\")\n",
        "            query = cursor.fetchall()\n",
        "            text_eng_results = [row[0] for row in query]\n",
        "            print(text_eng_results)\n",
        "            return text_eng_results\n",
        "        except Exception as e:  \n",
        "                print(f\"Error: {e}\")\n",
        "                conn.rollback()\n",
        "        return \"Not found any related.\"\n",
        "\n",
        "    def gpt3_sum(self, user_input, engine='gpt-35-turbo', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):\n",
        "        retry, max_retry = 0, 10\n",
        "        while retry < max_retry:\n",
        "            try:\n",
        "                setting = [{\"role\": \"system\", \"content\": \"Summary the user input as a bullet, use only Thai language\"}, \n",
        "                        {\"role\": \"user\", \"content\": f\"{user_input}\"}]\n",
        "                response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,\n",
        "                                                        top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen,\n",
        "                                                        messages=setting)\n",
        "                return response['choices'][0]['message']['content']\n",
        "            except openai.error.OpenAIError as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                retry += 1\n",
        "                if retry < max_retry:\n",
        "                    print(f\"Due to content filtering, retrying. (attempt {retry}/{max_retry})...\")\n",
        "                    time.sleep(3)  # Add a delay before retrying\n",
        "                else:\n",
        "                    print(\"Max retries reached. Exiting. Please change the question\")\n",
        "                    break\n",
        "        return response['choices'][0]['message']['content']\n",
        "    def chat(self, engine='gpt-4-32k', temp=0.5, top_p=0.95, tokens=4000, freq_pen=0.0, pres_pen=0.2):\n",
        "        retry, max_retry = 0, 10\n",
        "        while retry < max_retry:\n",
        "            try:\n",
        "                user_input = input(\"User:\")\n",
        "                print(\"User:\", user_input)\n",
        "                if user_input == \"stop\":\n",
        "                    retry += max_retry\n",
        "                    break\n",
        "                self.clear_memory()\n",
        "                self.update_memory('user', user_input)\n",
        "                lawyer_prompt_content = self.Lawyer_prompt + ''.join(self.search_vectordb(self.thai2eng(self.gpt3_sum(user_input))))\n",
        "                self.update_memory('system', lawyer_prompt_content)\n",
        "                response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,\n",
        "                                                        top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen,\n",
        "                                                        messages=self.memory)\n",
        "                self.update_memory('system', \"This is memory section that keep old conversation <memory>{mem}</memory>\"\n",
        "                    .format(mem = response['choices'][0]['message']['content']))\n",
        "                print(\"LawBot:\", response['choices'][0]['message']['content'])\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                retry += 1\n",
        "                if retry < max_retry:\n",
        "                    print(f\"Due to content filtering, retrying. (attempt {retry}/{max_retry})...\")\n",
        "                    time.sleep(3)  # Add a delay before retrying\n",
        "                else:\n",
        "                    print(\"Max retries reached. Exiting. Please change the question\")\n",
        "                    break\n",
        "                \n",
        "    def update_memory(self, role, content):\n",
        "        self.memory.append({\"role\": role, \"content\": content})\n",
        "        \n",
        "    def clear_memory(self):\n",
        "        self.memory = [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"\"}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "bot = Chatbot() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bot.chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1687787241694
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
