{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1687787597342
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# filepath ='/home/azureuser/cloudfiles/code/Users/supakit/SimilaritySearch/Mon/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1687787599042
        }
      },
      "outputs": [],
      "source": [
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"https://fusion-gpt-training.openai.azure.com/\"\n",
        "openai.api_version = \"2023-07-01-preview\"\n",
        "openai.api_key = \"a83037e20d3d4d7982b0c52080e730fb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1687787600702
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "host = \"pureelawvec.postgres.database.azure.com\"\n",
        "dbname = \"postgres\"\n",
        "user = \"pgadmin\"\n",
        "password = \"Puree1234\"\n",
        "sslmode = \"require\"\n",
        "# Construct connection string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1687787604150
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection established\n"
          ]
        }
      ],
      "source": [
        "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
        "conn = psycopg2.connect(conn_string) \n",
        "print(\"Connection established\")\n",
        "cursor = conn.cursor()\n",
        "# Fetch all rows from table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df = pd.read_csv(\"criminal-datasets.csv\")\n",
        "df['text'] = df['text'].astype(str) + df['notes'].astype(str)\n",
        "df['text'] = df['text'].str.replace('nan', '')\n",
        "df = df.drop(['article','notes'], axis = 1)\n",
        "df = df.drop(df.index[0:9])\n",
        "df = df.reset_index()\n",
        "df = df.drop('index', axis = 1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, uuid, json\n",
        "# You can pass more than one object in body.\n",
        "def thai2eng(input):\n",
        "    key = \"041e6b79b96146eabafaf524d78911be\"\n",
        "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "    location = \"southeastasia\"\n",
        "    path = '/translate'\n",
        "    constructed_url = endpoint + path\n",
        "\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': 'th',\n",
        "        'to': ['en']\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "        'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # translated = []\n",
        "    # for string in input:\n",
        "    body = [{\n",
        "        'text': input\n",
        "    }]\n",
        "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    # translated.append(response[0][\"translations\"][0]['text'])\n",
        "    # print(json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))\n",
        "    # print(response[0][\"translations\"][0]['text'])\n",
        "    return response[0][\"translations\"][0]['text']\n",
        "\n",
        "def eng2thai(input):\n",
        "    key = \"041e6b79b96146eabafaf524d78911be\"\n",
        "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "    location = \"southeastasia\"\n",
        "    path = '/translate'\n",
        "    constructed_url = endpoint + path\n",
        "\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': 'en',\n",
        "        'to': ['th']\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "        'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # translated = []\n",
        "    # for string in input:\n",
        "    body = [{\n",
        "        'text': input\n",
        "    }]\n",
        "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    return response[0][\"translations\"][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1687787606960
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"law.csv\")\n",
        "df['Content'] = df['Content'].str.replace(\"\\n\", \" \")\n",
        "# Extract sections using regular expressions\n",
        "pattern = r'Section \\d+\\..*?(?=Section \\d+\\.|$)'\n",
        "df['Section'] = df['Content'].apply(lambda x: re.findall(pattern, x, re.DOTALL))\n",
        "# Explode the list of sections into separate rows\n",
        "df = df.explode('Section')\n",
        "df_text = df.drop('Content', axis= 1)\n",
        "# # Reset index\n",
        "df_text.reset_index(drop=True, inplace=True)\n",
        "df = df_text.dropna()\n",
        "# for row in df_text.Content:\n",
        "#     if row < 20:\n",
        "#         df_text.drop(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687787676267
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.squeeze(df.Section.values)\n",
        "sentence_list = [x for x in x]\n",
        "sentence_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "translated = []\n",
        "for item in sentence_list:\n",
        "    translated.append(eng2thai(item))\n",
        "translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embed_list = []\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(sentence_list)\n",
        "embed_list.append(embeddings.tolist())\n",
        "print(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687787688675
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # cursor.execute(\"CREATE EXTENSION IF NOT EXISTS pg_trgm;\")\n",
        "    cursor.execute(\"DROP TABLE Thai_Law\")\n",
        "    cursor.execute(\"DROP TABLE Eng_Law\")\n",
        "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "    cursor.execute(\"\"\" \n",
        "    CREATE TABLE Criminal_Law(\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        embedding vector,\n",
        "        text_eng TEXT,\n",
        "        text_thai TEXT\n",
        "    );\"\"\")\n",
        "    for i in range(len(sentence_list)):  \n",
        "        cursor.execute(\"INSERT INTO Criminal_Law(id, embedding, text_eng, text_thai) VALUES (%s, %s, %s, %s);\",   \n",
        "                    (i, embed_list[0][i], sentence_list[i], translated[i]))\n",
        "        print(sentence_list[i]) \n",
        "        print(embed_list[0][i])\n",
        "        conn.commit()  \n",
        "except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        conn.rollback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1687787702719
        }
      },
      "outputs": [],
      "source": [
        "# def text2vec(input):\n",
        "#     # input = translator(input)\n",
        "#     embedding = openai.Embedding.create(input = input, engine = \"text-embedding-ada-002\")\n",
        "#     return embedding['data'][0]['embedding']\n",
        "\n",
        "def text2vec(input):\n",
        "    embed_list = []\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(input)\n",
        "    embed_list.append(embeddings.tolist())\n",
        "    return embed_list[0]\n",
        "\n",
        "def search_vectordb(question):\n",
        "    search_text = []\n",
        "    try:\n",
        "         cursor.execute(f\"\"\"SELECT text_thai, cosine_distance('{text2vec(question)}', embedding) as cosign\n",
        "         FROM Criminal_Law\n",
        "         ORDER BY cosine_distance('{text2vec(question)}', embedding) ASC LIMIT 7\n",
        "         \"\"\")\n",
        "         query = cursor.fetchall()\n",
        "         text_eng_results = [row[0] for row in query] \n",
        "         return text_eng_results\n",
        "    except Exception as e:  \n",
        "            print(f\"Error: {e}\")\n",
        "            conn.rollback()\n",
        "    return \"null\"\n",
        "\n",
        "#WHERE cosine_distance('{text2vec(question)}', embedding) < {start}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lawyer_prompt = \"LawBot, a GPT-based AI assistant designed specifically for answering legal questions and explaining complex legal terms.\\\n",
        "        Analysis the Question sector and provide a comprehensive and briefly answer in Answer section.The answer must address all aspects\\\n",
        "        of the Question and support your argument with appropriate legal references in Context section. Only answered by thai language only \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "setting = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":\"\"}]\n",
        "def gpt3_completion(user_input, engine='gpt-35-turbo-16k', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):\n",
        "    setting.append({'role':'system',\"content\":  \n",
        "      Lawyer_prompt + ''.join(search_vectordb(thai2eng(gpt3_sum(user_input))))})\n",
        "    # setting.append({'role':'user',\"content\":f\"{gpt3_sum(user_input)}\"})\n",
        "    response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,\n",
        "      top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen, messages = setting)\n",
        "    print(\"LawBot:\", response['choices'][0]['message']['content'])\n",
        "    \n",
        "def gpt3_sum(user_input, engine='gpt-35-turbo', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):\n",
        "    setting.append({'role':'system',\"content\":\"Summary the user input as a bullet , use only thai language\"})\n",
        "    setting.append({'role':'user',\"content\":f\"{user_input}\"})\n",
        "    response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,\n",
        "      top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen, messages = setting)\n",
        "    # print(\"SumBot:\", response['choices'][0]['message']['content'])\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# while True:\n",
        "#   # เผาบ้านเรือนมีความผิดตามข้อไหน\n",
        "#   # ป้องกันตัวเองอย่างไรไม่ให้ผิดกฏหมาย\n",
        "#   user_input = input()\n",
        "#   print(\"User:\", user_input)\n",
        "#   if user_input == \"quit\":\n",
        "#     break\n",
        "#   response = gpt3_completion(thai2eng(user_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = gpt3_sum(\"ป้องกันตัวเองอย่างไรไม่ให้ผิดกฏหมาย\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LawBot: ในกรณีที่คุณมีความคิดอยากฆ่าคน คุณต้องระวางโทษตามกฎหมายไทย ภายใต้มาตรา 290 ของประมวลกฎหมายอาญา ผู้ที่ทำให้ผู้อื่นถึงแก่ความตายโดยทำร้ายร่างกายของบุคคลนั้นโดยไม่มีเจตนาทำให้ผู้อื่นถึงแก่ความตาย จะต้องถูกลงโทษจำคุกตั้งแต่ 3 ปีถึง 15 ปี อย่างไรก็ตาม หากการกระทำของคุณเป็นผลให้เกิดความเสียหายต่อผู้อื่นโดยร้ายแรง คุณอาจถูกลงโทษจำคุกตั้งแต่ 1 ปีถึง 20 ปี และต้องรับโทษปรับตั้งแต่ 2,000 บาทถึง 20,000 บาท อีกทั้งหากการกระทำของคุณเป็นเหตุให้ผู้อื่นถึงแก่ความตาย คุณอาจถูกลงโทษจำคุกตั้งแต่ 5 ปีถึง 20 ปี และต้องรับโทษปรับตั้งแต่ 10,000 บาทถึง 40,000 บาท ดังนั้น ควรปฏิบัติตามกฎหมายอย่างถูกต้องและปฏิบัติตามหลักการและค่านิยมที่ดี เพื่อป้องกันตัวเองไม่ให้ผิดกฎหมาย\n"
          ]
        }
      ],
      "source": [
        "def update_setting(role, content):  \n",
        "    setting.append({\"role\": role, \"content\": content})  \n",
        "  \n",
        "def get_response_from_gpt3(user_input, engine='gpt-35-turbo-16k', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):  \n",
        "    update_setting('user', user_input)  \n",
        "    lawyer_prompt_content = Lawyer_prompt + ''.join(search_vectordb(thai2eng(gpt3_sum(user_input))))  \n",
        "    update_setting('system', lawyer_prompt_content)  \n",
        "  \n",
        "    response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,  \n",
        "      top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen, messages = setting)  \n",
        "    return response['choices'][0]['message']['content']  \n",
        "  \n",
        "# usage  \n",
        "user_input = \"อยากฆ่าคนทำได้ไหม\"  \n",
        "print(\"LawBot:\", get_response_from_gpt3(user_input))  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "class Chatbot():\n",
        "    def __init__(self):\n",
        "        self.memory = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":\"\"}]\n",
        "    # def check_memory_file(self):  \n",
        "    #     if os.path.isfile(\"./memory.txt\"):  \n",
        "    #         pass  \n",
        "    #     else:  \n",
        "    #         with open(\"./memory.txt\", \"w\", encoding='utf-8') as mem:  \n",
        "    #             mem.write(x)\n",
        "    #             mem.close()\n",
        "    # def clear_memory(self):\n",
        "    #     # file_path = \"/path/to/your/file.txt\"  \n",
        "    #     if os.path.isfile(\"./memory.txt\"):  \n",
        "    #         os.remove(\"./memory.txt\")  \n",
        "    #     else:\n",
        "    #         print(\"Chatbot does not have memory.\")\n",
        "    def update_memory(self, role, content):  \n",
        "        self.memory.append({\"role\": role, \"content\": content}) \n",
        "        \n",
        "    def gpt3_sum(user_input, engine='gpt-35-turbo', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):\n",
        "        setting = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":\"\"}]\n",
        "        setting.append({'role':'system',\"content\":\"Summary the user input as a bullet , use only thai language\"})\n",
        "        setting.append({'role':'user',\"content\":f\"{user_input}\"})\n",
        "        response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,\n",
        "        top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen, messages = setting)\n",
        "        return response['choices'][0]['message']['content']  \n",
        "\n",
        "    def gpt(user_input, engine='gpt-35-turbo-16k', temp=0.5, top_p=0.95, tokens=2000, freq_pen=0.0, pres_pen=0.2):\n",
        "        update_setting('user', user_input)  \n",
        "        lawyer_prompt_content = Lawyer_prompt + ''.join(search_vectordb(thai2eng(gpt3_sum(user_input))))  \n",
        "        update_setting('system', lawyer_prompt_content)  \n",
        "    \n",
        "        response = openai.ChatCompletion.create(engine=engine, temperature=temp, max_tokens=tokens,  \n",
        "        top_p=top_p, frequency_penalty=freq_pen, presence_penalty=pres_pen, messages = setting)  \n",
        "        print(response['choices'][0]['message']['content'])\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.setting = [{\"role\":\"system\",\"content\":\"\"},{\"role\":\"user\",\"content\":\"\"}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "bot = Chatbot()  \n",
        "bot.check_memory_file() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "bot.clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1687787241694
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
